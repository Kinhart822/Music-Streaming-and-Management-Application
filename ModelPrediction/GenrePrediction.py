import pandas as pd
import json
import numpy as np
import subprocess
import whisper
import hashlib
import os
import re
import contractions
import spacy
import inflect
import tempfile
from io import BytesIO
from collections import defaultdict
from lingua import Language, LanguageDetectorBuilder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sqlalchemy import create_engine, text
import psycopg2
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Additional
conn = psycopg2.connect(
    dbname="music_management",
    user="postgres",
    password="kinhart822",
    host="localhost",
    port="5432"
)

nlp = spacy.load('en_core_web_sm')

p = inflect.engine()  # T·∫°o engine ƒë·ªÉ chuy·ªÉn s·ªë th√†nh ch·ªØ

directories = [
    r"D:\Pycharm_Projects\Python_Projects\music\copyrighted",
    r"D:\Pycharm_Projects\Python_Projects\music\no_copyrighted"
]

engine = create_engine("postgresql+psycopg2://postgres:kinhart822@localhost:5432/music_management")


def fetch_songs():
    query = "SELECT * FROM public.songs ORDER BY id ASC;"
    return pd.read_sql_query(query, conn)


def detect_languages(text, detector):
    """Ph√°t hi·ªán ng√¥n ng·ªØ v√† x√°c su·∫•t c·ªßa ƒëo·∫°n vƒÉn b·∫£n."""
    if isinstance(text, str) and text.strip():
        confidences = detector.compute_language_confidence_values(text)
        return {lang.language.name: lang.value for lang in confidences}
    return {}


def detect_languages_for_dataframe(df):
    languages = [
        Language.ENGLISH, Language.VIETNAMESE, Language.KOREAN, Language.CHINESE, Language.JAPANESE,
        Language.FRENCH, Language.GERMAN, Language.SPANISH, Language.ITALIAN, Language.RUSSIAN,
        Language.PORTUGUESE, Language.POLISH
    ]

    detector = LanguageDetectorBuilder.from_languages(*languages).build()
    results = []

    for _, row in df.dropna().iterrows():
        text = row["lyrics"]
        segments = [line for line in text.strip().splitlines() if line.strip()]

        language_totals = defaultdict(float)
        segment_count = 0

        for segment in segments:
            detected_languages = detect_languages(segment, detector)
            for lang, value in detected_languages.items():
                if lang != "UNKNOWN":
                    language_totals[lang] += value
            segment_count += 1

        if not language_totals:
            results.append({
                "id": row["id"],
                "title": row["title"],
                "artist": row["artist"],
                "lyrics": text,
                "detected_languages": "Unknown",
                "multilingual": "Unknown"
            })
            continue

        average_languages = {lang: round(value / segment_count * 100, 2) for lang, value in language_totals.items()}

        multilingual = any(
            lang in {"VIETNAMESE", "KOREAN", "CHINESE", "JAPANESE", "FRENCH", "GERMAN", "SPANISH", "ITALIAN", "RUSSIAN",
                     "PORTUGUESE", "POLISH"} and value > 20
            for lang, value in average_languages.items()
        )

        results.append({
            "id": row["id"],
            "title": row["title"],
            "artist": row["artist"],
            "lyrics": text,
            "detected_languages": average_languages,
            "multilingual": "Yes" if multilingual else "No"
        })

    return pd.DataFrame(results)


df_results = detect_languages_for_dataframe(df)
non_english_songs = df_results[df_results["multilingual"] == "Yes"]
non_english_ids = non_english_songs["id"].tolist()

if non_english_ids:
    with engine.connect() as conn:
        for song_id in non_english_ids:
            query = text("DELETE FROM song WHERE id = :id")
            conn.execute(query, {"id": song_id})
        conn.commit()
else:
    print("‚úÖ D·ªØ li·ªáu ƒë√£ ch·ªâ ch·ª©a b√†i h√°t ti·∫øng Anh, kh√¥ng c·∫ßn x√≥a.")

columns_to_keep = ["id", "title", "lyrics", "artist", "label_copyrighted"]
df = df[columns_to_keep]


def load_contractions(file_path):
    contractions_dict = {}
    with open(file_path, "r", encoding="utf-8") as file:
        for line in file:
            short, full = line.strip().split("=")
            contractions_dict[short] = full
    return contractions_dict


def load_remove(file_path):
    remove_list = set()
    with open(file_path, "r", encoding="utf-8") as file:
        for line in file:
            word = line.strip()
            if word:
                remove_list.add(re.escape(word))
    return remove_list


# Convert to lowercase, strip and remove punctuations
def preprocess(text):
    if pd.isna(text):
        return ""

    lyrics = text.lower().strip()

    lyrics = re.sub(r'\d+', lambda x: p.number_to_words(x.group()), lyrics)

    custom_remove = load_remove("custom_remove.txt")
    pattern = r"\b(" + "|".join(custom_remove) + r")\b"
    lyrics = re.sub(pattern, "", lyrics, flags=re.IGNORECASE)

    lyrics = lyrics.replace("in'", "ing").replace("in '", "ing")
    lyrics = lyrics.replace("in‚Äô", "ing").replace("in ‚Äô", "ing")

    custom_contractions = load_contractions("custom_contractions.txt")
    for short, full in custom_contractions.items():
        lyrics = lyrics.replace(short, full)

    lyrics = contractions.fix(lyrics)
    lyrics = lyrics.replace("'", "").replace("`", "").replace("‚Äô", "")
    lyrics = re.sub(r'<.*?>', '', lyrics)
    lyrics = re.sub(r'\[[0-9]*\]', '', lyrics)
    lyrics = re.sub(r"[,.:?\[\]{}\-+\\/|@#$*^&%\~!()\";]", "", lyrics)
    lyrics = re.sub(r"\s+", " ", lyrics).strip()

    return lyrics


def hash_column(series):
    """T·∫°o hash t·ª´ d·ªØ li·ªáu c·ªßa c·ªôt ƒë·ªÉ ki·ªÉm tra thay ƒë·ªïi"""
    return hashlib.md5(series.to_string().encode()).hexdigest()


def check_exist(df):
    if not os.path.exists("clean_lyrics_stopwords_lemmatizer.csv"):
        print("üìÇ L·∫ßn ƒë·∫ßu ch·∫°y: T·∫°o m·ªõi file CSV...")
    else:
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ CSV
        df_old = pd.read_csv("clean_lyrics_stopwords_lemmatizer.csv")

        # Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ thay ƒë·ªïi kh√¥ng
        if "clean_lyrics" in df_old and hash_column(df["lyrics"]) == hash_column(df_old["lyrics"]):
            print("‚úÖ Kh√¥ng c√≥ thay ƒë·ªïi, t·∫£i d·ªØ li·ªáu t·ª´ CSV...")
            df["clean_lyrics"] = df_old["clean_lyrics"]
            return df

        print("üîÑ D·ªØ li·ªáu thay ƒë·ªïi, x√≥a CSV c≈© v√† t·∫°o l·∫°i...")
        os.remove("clean_lyrics_stopwords_lemmatizer.csv")

    # Ch·∫°y preprocessing v√† l∆∞u v√†o CSV
    print("üöÄ ƒêang x·ª≠ l√Ω lyrics...")
    df["clean_lyrics"] = df["lyrics"].apply(preprocess)
    df.to_csv("clean_lyrics_stopwords_lemmatizer.csv", index=False)
    return df


# Train the model prediction
df = check_exist(df)
df_train = df
X_train, X_test, y_train, y_test = train_test_split(df_train["clean_lyrics"], df_train["label_copyrighted"],
                                                    test_size=0.2, shuffle=True, random_state=42)
vectorizer_tfidf = TfidfVectorizer(use_idf=True)
X_train_vectors_tfidf = vectorizer_tfidf.fit_transform(X_train)
X_test_vectors_tfidf = vectorizer_tfidf.transform(X_test)



def load_hyperparams():
    """Load hyperparameters t·ª´ file JSON"""
    try:
        with open("best_hyperparams_stopwords_lemmatizer.json", "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return None


def save_hyperparams(best_params):
    """L∆∞u hyperparameters v√†o file JSON"""
    best_params = {key: int(value) if isinstance(value, np.integer) else value for key, value in best_params.items()}
    with open("best_hyperparams_stopwords_lemmatizer.json", "w") as f:
        json.dump(best_params, f, indent=4)


def find_best_hyperparams(X_train, y_train):
    print("üîç T√¨m hyperparameters t·ªëi ∆∞u v·ªõi GridSearchCV...")

    param_grid = {
        'n_estimators': np.arange(50, 500, 50),
        'max_depth': [5, 10, 15, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2'],
        'bootstrap': [True, False],
    }

    rf = RandomForestClassifier()
    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=5,
        verbose=2,
        n_jobs=-1
    )
    grid_search.fit(X_train, y_train)

    best_params = grid_search.best_params_
    print("üéØ Best Parameters:", best_params)

    # L∆∞u hyperparameters v√†o JSON
    save_hyperparams(best_params)
    return best_params


def check_and_update_hyperparams(X_train, y_train, df):
    best_params = load_hyperparams()

    if best_params:
        print("‚úÖ Hyperparameters ƒë√£ c√≥ s·∫µn, ki·ªÉm tra d·ªØ li·ªáu...")
        df_old = pd.read_csv("clean_lyrics_stopwords_lemmatizer.csv")

        if hash_column(df["lyrics"]) == hash_column(df_old["lyrics"]):
            print("‚úÖ D·ªØ li·ªáu kh√¥ng ƒë·ªïi, gi·ªØ nguy√™n best_params.")
            return best_params

        print("üîÑ D·ªØ li·ªáu thay ƒë·ªïi, t√≠nh l·∫°i hyperparameters...")

    else:
        print("üìÇ L·∫ßn ƒë·∫ßu ch·∫°y: T·∫°o m·ªõi file JSON...")

    return find_best_hyperparams(X_train, y_train)


best_params_for_rf = check_and_update_hyperparams(X_train_vectors_tfidf, y_train, df)
rf = RandomForestClassifier(**best_params_for_rf, class_weight="balanced", random_state=42)


def transcribe_lyric_by_whisper(audio_path):
    try:
        print("ƒêang chuy·ªÉn ƒë·ªïi audio th√†nh text b·∫±ng Whisper...")
        model_to_load = whisper.load_model("large")
        result = model_to_load.transcribe(audio_path, fp16=False)
        lyrics = result["text"]

        print("üé§ L·ªùi b√†i h√°t ƒë∆∞·ª£c tr√≠ch xu·∫•t th√†nh c√¥ng!")
        return lyrics
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t l·ªùi b√†i h√°t: {e}")
        return None


def separate_vocals(input_file, output_folder="demucs_output"):
    """Ch·∫°y Demucs ƒë·ªÉ t√°ch gi·ªçng h√°t t·ª´ file MP3 v√† ƒë·ªïi t√™n file k·∫øt qu·∫£."""

    # Ki·ªÉm tra file ƒë·∫ßu v√†o c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(input_file):
        print(f"‚ùå L·ªói: File ƒë·∫ßu v√†o '{input_file}' kh√¥ng t·ªìn t·∫°i!")
        return None

    try:
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(output_folder, exist_ok=True)

        filename = os.path.splitext(os.path.basename(input_file))[0]
        vocals_dir = os.path.join(output_folder, "mdx_extra_q", filename)
        vocals_path = os.path.join(vocals_dir, "vocals.mp3")
        new_vocals_path = os.path.join(vocals_dir, f"{filename}_vocals.mp3")

        if os.path.exists(new_vocals_path):
            print(f"‚úÖ File vocals ƒë√£ t·ªìn t·∫°i: {new_vocals_path}")
            return new_vocals_path

        # Ch·∫°y Demucs ƒë·ªÉ t√°ch gi·ªçng h√°t
        command = [
            "demucs", "-o", output_folder, "--mp3",
            "-n", "mdx_extra_q", input_file
        ]
        subprocess.run(command, check=True)

        print(f"‚úÖ T√°ch gi·ªçng h√°t th√†nh c√¥ng! K·∫øt qu·∫£ l∆∞u t·∫°i: {output_folder}")

        # Ki·ªÉm tra file vocals.mp3 c√≥ ƒë∆∞·ª£c t·∫°o ra kh√¥ng
        if not os.path.exists(vocals_path):
            print(f"‚ùå L·ªói: File vocals.mp3 kh√¥ng t·ªìn t·∫°i trong th∆∞ m·ª•c {vocals_dir}")
            return None

        # ƒê·ªïi t√™n file
        os.rename(vocals_path, new_vocals_path)
        print(f"‚úÖ ƒê√£ ƒë·ªïi t√™n file th√†nh: {new_vocals_path}")

        return new_vocals_path

    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói khi ch·∫°y Demucs: {e}")
        return None


def slow_down_audio_ffmpeg(input_file, output_folder="final_output", speed_factor=0.85):
    """L√†m ch·∫≠m audio b·∫±ng FFmpeg v√† lo·∫°i b·ªè kho·∫£ng l·∫∑ng"""
    try:
        # T√°ch t√™n file kh√¥ng c√≥ ph·∫ßn m·ªü r·ªông
        filename = os.path.splitext(os.path.basename(input_file))[0]

        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(output_folder, exist_ok=True)

        # T·∫°o ƒë∆∞·ªùng d·∫´n cho file ƒë√£ l√†m ch·∫≠m
        slowed_audio = os.path.join(output_folder, f"{filename}_slowdown.wav")

        # L·ªánh FFmpeg l√†m ch·∫≠m audio
        command = [
            "ffmpeg", "-i", input_file,
            "-filter:a", f"atempo={speed_factor}", "-vn",
            "-y", slowed_audio  # Ghi ƒë√® file n·∫øu ƒë√£ t·ªìn t·∫°i
        ]
        subprocess.run(command, check=True)

        # G·ªçi h√†m remove_silence ƒë·ªÉ x·ª≠ l√Ω file ƒë√£ l√†m ch·∫≠m
        final_output = os.path.join(output_folder, f"{filename}_final.wav")
        result = remove_silence(slowed_audio, final_output)

        if os.path.exists(slowed_audio):
            os.remove(slowed_audio)

        return result
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói khi ch·∫°y FFmpeg: {e}")
        return None


def remove_silence(input_audio, output_audio, silence_threshold="-40dB", min_silence_duration="0.5"):
    """Lo·∫°i b·ªè kho·∫£ng l·∫∑ng trong file audio b·∫±ng FFmpeg"""
    try:
        command = [
            "ffmpeg",
            "-i", input_audio,
            "-af",
            f"silenceremove=stop_periods=-1:stop_duration={min_silence_duration}:stop_threshold={silence_threshold}",
            "-y",
            output_audio
        ]
        subprocess.run(command, check=True)
        return output_audio
    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói khi lo·∫°i b·ªè kho·∫£ng l·∫∑ng: {e}")
        return None


def detect_languages(text, detector):
    """Ph√°t hi·ªán ng√¥n ng·ªØ v√† x√°c su·∫•t c·ªßa ƒëo·∫°n vƒÉn b·∫£n."""
    if isinstance(text, str) and text.strip():
        confidences = detector.compute_language_confidence_values(text)
        return {lang.language.name: lang.value for lang in confidences}
    return {}


def detect_languages_for_lyrics(lyrics):
    languages = [
        Language.ENGLISH, Language.VIETNAMESE, Language.KOREAN, Language.CHINESE, Language.JAPANESE, Language.FRENCH,
        Language.GERMAN, Language.SPANISH, Language.ITALIAN, Language.RUSSIAN, Language.PORTUGUESE, Language.POLISH,
    ]

    detector = LanguageDetectorBuilder.from_languages(*languages).build()

    text = lyrics.strip()
    segments = [line for line in text.splitlines() if line.strip()]

    if not segments:
        return {"detected_languages": "Unknown", "multilingual": "Unknown"}

    language_totals = defaultdict(float)
    segment_count = len(segments)

    for segment in segments:
        detected_languages = detect_languages(segment, detector)
        for lang, value in detected_languages.items():
            if lang != "UNKNOWN":
                language_totals[lang] += value

    if not language_totals:
        return {"detected_languages": "Unknown", "multilingual": "Unknown"}

    # Calculate the average confidence percentages
    average_languages = {lang: round(value / segment_count * 100, 2) for lang, value in language_totals.items()}

    # Check if text is multilingual (if any language has > 20% confidence)
    multilingual = any(
        lang in {"VIETNAMESE", "KOREAN", "CHINESE", "JAPANESE", "FRENCH", "GERMAN", "SPANISH", "ITALIAN", "RUSSIAN",
                 "PORTUGUESE", "POLISH"} and value > 20
        for lang, value in average_languages.items()
    )

    return {
        "detected_languages": average_languages,
        "multilingual": "Yes" if multilingual else "No"
    }


def preprocess_lyrics(text, multilingual):
    if multilingual == "No":
        lyrics = preprocess(text)
        return lyrics
    return "Kh√¥ng h·ª£p l·ªá: VƒÉn b·∫£n ch·ª©a nhi·ªÅu ng√¥n ng·ªØ."





def find_similar_lyrics_with_tf_idf(lyrics, vectorizer, tfidf_matrix):
    if not lyrics:
        print("üö´ Kh√¥ng th·ªÉ so s√°nh do l·ªói khi tr√≠ch xu·∫•t lyrics.")
        return None

    lyrics_data = detect_languages_for_lyrics(lyrics)
    preprocessed_lyrics = preprocess_lyrics(lyrics, lyrics_data["multilingual"])

    if preprocessed_lyrics != "Kh√¥ng h·ª£p l·ªá: VƒÉn b·∫£n ch·ª©a nhi·ªÅu ng√¥n ng·ªØ.":
        vector = vectorizer.transform([preprocessed_lyrics])
        similarity_scores = cosine_similarity(vector, tfidf_matrix)[0]

        # T√¨m index c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t
        best_index = similarity_scores.argmax()
        best_score = similarity_scores[best_index]

        # L·∫•y th√¥ng tin b√†i h√°t t∆∞∆°ng ·ª©ng
        best_match = df.iloc[best_index]

        result = {
            "Title": best_match["title"],
            "Lyrics": best_match["lyrics"],
            "Similarity": best_score
        }

        return result

    return "Kh√¥ng h·ª£p l·ªá: VƒÉn b·∫£n ch·ª©a nhi·ªÅu ng√¥n ng·ªØ."


def process_audio(input_file):
    # T·∫°o file t·∫°m ƒë·ªÉ l∆∞u d·ªØ li·ªáu t·ª´ file object
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio:
        temp_audio.write(input_file.read())  # Ghi d·ªØ li·ªáu v√†o file t·∫°m
        temp_audio_path = temp_audio.name  # L∆∞u ƒë∆∞·ªùng d·∫´n file t·∫°m

    print(f"üîÑ ƒê√£ l∆∞u file t·∫°m: {temp_audio_path}")

    # G·ªçi h√†m ƒë·ªÉ t√°ch vocals t·ª´ file t·∫°m
    vocals_path = separate_vocals(temp_audio_path)

    if vocals_path and os.path.exists(vocals_path):
        processed_audio = slow_down_audio_ffmpeg(vocals_path)

        # X√≥a file t·∫°m sau khi x·ª≠ l√Ω xong
        os.remove(temp_audio_path)
        os.remove(vocals_path)

        if processed_audio:
            print(f"üéµ Audio ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng: {processed_audio}")
            return processed_audio
        else:
            print(f"‚ùå L·ªói khi l√†m ch·∫≠m audio cho {vocals_path}")
            return None

    print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file vocals t·∫°i {vocals_path}")
    os.remove(temp_audio_path)
    return None


def predict_copyright(model_prediction, tfidf_vector):
    prob = model_prediction.predict_proba(tfidf_vector)[0][1]
    prediction_label = "‚ö†Ô∏è Copyrighted" if prob >= 0.5 else "‚úÖ No Copyrighted"

    return prediction_label


def tfidf_lyrics(lyrics):
    if not lyrics:
        print("üö´ Kh√¥ng c√≥ lyrics.")
        return None

    lyrics_data = detect_languages_for_lyrics(lyrics)
    preprocessed_lyrics = preprocess_lyrics(lyrics, lyrics_data["multilingual"])

    if preprocessed_lyrics != "Kh√¥ng h·ª£p l·ªá: VƒÉn b·∫£n ch·ª©a nhi·ªÅu ng√¥n ng·ªØ.":
        # Bi·∫øn lyrics th√†nh vector TF-IDF
        remix_song_tfidf = vectorizer_tfidf.transform([preprocessed_lyrics])
        return remix_song_tfidf

    return "Kh√¥ng h·ª£p l·ªá: VƒÉn b·∫£n ch·ª©a nhi·ªÅu ng√¥n ng·ªØ."


def main():
    df = fetch_songs()

    vectorizer = TfidfVectorizer(use_idf=True)
    tfidf_matrix = vectorizer.fit_transform(df["clean_lyrics"])

    lyrics_provided = input()
    tf_lyric = tfidf_lyrics(lyrics_provided)
    result_prediction_provided_lyrics = predict_copyright(rf, tf_lyric)
    result_similarity_provided_lyrics = find_similar_lyrics_with_tf_idf(lyrics_provided, vectorizer, tfidf_matrix)
    if result_prediction_provided_lyrics == "‚úÖ No Copyrighted":
        if result_similarity_provided_lyrics and result_similarity_provided_lyrics["Similarity"] >= 0.5:
            print("üö´ B√†i h√°t ƒë√£ t·ªìn t·∫°i!")
        else:
            print("‚úÖ B√†i h√°t ch∆∞a t·ªìn t·∫°i, b·∫°n c√≥ th·ªÉ ƒëƒÉng nh·∫°c c·ªßa m√¨nh l√™n!")
    elif result_prediction_provided_lyrics == "‚ö†Ô∏è Copyrighted" and result_similarity_provided_lyrics["Similarity"] < 0.5:
        print("‚úÖ B√†i h√°t ko vi ph·∫°m b·∫£n quy·ªÅn v√† ch∆∞a t·ªìn t·∫°i, b·∫°n c√≥ th·ªÉ ƒëƒÉng nh·∫°c c·ªßa m√¨nh l√™n!")
    else:
        print("‚úÖ B√†i h√°t ƒë√£ vi ph·∫°m b·∫£n quy·ªÅn!")


    # input_file =
    # processed_audio = process_audio(input_file)
    # lyrics_whisper = transcribe_lyric_by_whisper(processed_audio)
    # tf_lyric_audio = tfidf_lyrics(lyrics_whisper)
    # result_prediction_audio_lyrics = predict_copyright(rf_best, lyrics_whisper)
    # if result_prediction_audio_lyrics == "‚úÖ No Copyrighted":
    #     result_similarity_audio_lyrics = find_similar_lyrics_with_tf_idf(lyrics_whisper, vectorizer, tfidf_matrix)
    #     if result_similarity_audio_lyrics and result_similarity_audio_lyrics["Similarity"] >= 0.5:
    #         print("üö´ B√†i h√°t ƒë√£ t·ªìn t·∫°i!")
    #     else:
    #         print("‚úÖ B√†i h√°t ch∆∞a t·ªìn t·∫°i, b·∫°n c√≥ th·ªÉ ƒëƒÉng nh·∫°c c·ªßa m√¨nh l√™n!")


if __name__ == "__main__":
    main()
